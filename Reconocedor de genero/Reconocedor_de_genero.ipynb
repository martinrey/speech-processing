{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento del habla\n",
    "Primero, vamos a importar los paquetes necesarios para el desarrollo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import plotly\n",
    "import wav\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from IPython.display import Audio\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Dropout, Flatten\n",
    "\n",
    "from plotly.graph_objs import Scatter, Layout, Bar, Figure\n",
    "\n",
    "plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VERSION 1: RECONOCIMIENTO UTILIZANDO ÚNICAMENTE LA ONDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enunciado del problema: dados 184 audios clasificados, se pide crear un clasificador que detecte si el hablante es de sexo mujer o hombre.\n",
    "Analisis del problema: el audio mas corto posee 318764 frames. Es decir, estaríamos manejando una matriz de X con dimension (184, 318764). En general, para crear un clasificador que generalice bien necesitamos que m >> n donde (m, n). Esto quiere decir, que tengamos muchas mas instancias que cantidad de parametros. Lo que suele suceder al tener pocas instancias con muchos parámetros es que en vez de generalizar, terminamos produciendo mucho overfitting (nos pegamos mucho a las enormes cantidades de parametros y tenemos muy pocas muestras distintas).\n",
    "\n",
    "Un primer approach va a ser aumentar nuestro set de datos. El audio mas corto dura 19 segundos. Lo que vamos a hacer es a cada audio cortarlo en pedazos de 4 segundos cada uno. Esto viene de que lo que nos importa es capturar los patrones en la voz que nos permitan definir si es hombre o mujer, no nos interesa la secuencia de palabras que dice.\n",
    "\n",
    "Primero vamos a definir nuestor set de entrenamiento y testeo y luego extenderlos.\n",
    "\n",
    "Comencemos primero definiendo algunas variables globales (parámetros del enunciado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RUTA_DIRECTORIO_DATOS = \"datos\"\n",
    "VENTANA_EN_SEGUNDOS = 4\n",
    "SAMPLE_RATE = 16000\n",
    "MINIMA_CANTIDAD_DE_FRAMES_A_PROCESAR = 318764 # Es el minimo de todos en este caso\n",
    "CANTIDAD_DE_FRAMES_A_PROCESAR = VENTANA_EN_SEGUNDOS * SAMPLE_RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a cargar los archivos IPU y WAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "archivos_en_carpeta_datos = os.listdir(RUTA_DIRECTORIO_DATOS)\n",
    "archivos_wav = []\n",
    "archivos_ipu = []\n",
    "for archivo in archivos_en_carpeta_datos:\n",
    "    if (archivo[-3:] == \"ipu\"):\n",
    "        archivos_ipu.append(archivo)\n",
    "    elif (archivo[-3:] == \"wav\"):\n",
    "        archivos_wav.append(archivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo mi set de datos (X's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "leng = []\n",
    "for archivo_wav in archivos_wav:\n",
    "    data, frames, _, duration = wav.load_from_wav(RUTA_DIRECTORIO_DATOS + \"/\" + archivo_wav)\n",
    "    leng.append(len(data))\n",
    "    X.append(data[:MINIMA_CANTIDAD_DE_FRAMES_A_PROCESAR])\n",
    "\n",
    "X = np.asarray(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo las etiquetas (mis Y's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = []\n",
    "y_sgd = []\n",
    "for archivo_wav in archivos_wav:\n",
    "    if archivo_wav[3] == \"m\":\n",
    "        y.append(1)\n",
    "    elif archivo_wav[3] == \"f\":\n",
    "        y.append(0)\n",
    "y = np.asarray(y)\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testeo que los datos hayan sido cargados correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Audio(data=X[25], rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separo en un set de entrenamiento y testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y, random_state=42)\n",
    "X_train = X[:150]\n",
    "X_test = X[150:180]\n",
    "y_train = y[:150]\n",
    "y_test = y[150:180]\n",
    "\n",
    "print(\"Shape X_train {}\".format(np.shape(X_train)))\n",
    "print(\"Shape X_test {}\".format(np.shape(X_test)))\n",
    "print(\"Shape y_train {}\".format(np.shape(y_train)))\n",
    "print(\"Shape y_test {}\".format(np.shape(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrando los set de datos para evitar overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augument_data(dataset, labels):\n",
    "    augumented_dataset = []\n",
    "    augumented_labels = []\n",
    "\n",
    "    for data, label in zip(dataset, labels):\n",
    "        lower_bound = 0\n",
    "        upper_bound = CANTIDAD_DE_FRAMES_A_PROCESAR\n",
    "        augumented_data = []\n",
    "        # corto en 4 pedazos el audio\n",
    "        for i in range(4):\n",
    "            augumented_data.append(data[lower_bound:upper_bound])\n",
    "            augumented_labels.append(label)\n",
    "            lower_bound = upper_bound\n",
    "            upper_bound += CANTIDAD_DE_FRAMES_A_PROCESAR\n",
    "        augumented_dataset.extend(augumented_data)\n",
    "\n",
    "    augumented_dataset = np.asarray(augumented_dataset)\n",
    "    augumented_labels = np.asarray(augumented_labels)\n",
    "    return augumented_dataset, augumented_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = augument_data(X_train, y_train)\n",
    "X_test, y_test = augument_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Shape X_train {}\".format(np.shape(X_train)))\n",
    "print(\"Shape X_test {}\".format(np.shape(X_test)))\n",
    "print(\"Shape y_train {}\".format(np.shape(y_train)))\n",
    "print(\"Shape y_test {}\".format(np.shape(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos nuestro perceptron multicapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=64, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(2, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definimos nuestra red neuronal convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    # CREO MIS CONVOLUCIONES\n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(16, 5, strides=1, padding='same', kernel_initializer=init, activation='relu', \n",
    "                            input_shape=(64000, 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Convolution1D(32, 5, strides=1, padding='same', kernel_initializer=init, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Convolution1D(64, 5, strides=1, padding='same', kernel_initializer=init, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Convolution1D(128, 5, strides=1, padding='same', kernel_initializer=init, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Convolution1D(256, 5, strides=1, padding='same', kernel_initializer=init, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(256, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(128, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el siguiente paso es definir nuestro preprocesador de la entrada y nuestro clasificador. Para esta tarea vamos a utilizar Pipelines que se encargan de enviar mensajes de manera secuencial a los objetos que definamos en el Pipeline. En este caso vamos a pedirle el preproceso al objeto encargado de calcular las componentes principales, las cuales van a alimentar a nuestro clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Reshape(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.reshape((X.shape[0], X.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mlp_model = KerasClassifier(build_fn=mlp_model, verbose=0)\n",
    "conv_model = KerasClassifier(build_fn=conv_model, verbose=0)\n",
    "\n",
    "estimators = [(\"reshaper\", Reshape()), ('clf', conv_model)]\n",
    "pipeline = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar GridSearch para probar distintas configuraciones de parámetros sobre PCA y la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OPTIMIZERS = ['adam']\n",
    "EPOCHS = [10]\n",
    "BATCHES = [10]\n",
    "INIT = ['glorot_uniform']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos nuestro diccionario en el formato que toma GridSearchCV para efectivamente ejecutar las diferentes configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'clf__optimizer': OPTIMIZERS,\n",
    "        'clf__epochs': EPOCHS,\n",
    "        'clf__batch_size': BATCHES,\n",
    "        'clf__init': INIT\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver que también generaliza nuestro clasificador vamos a utilizar KFold Cross-Validation, en particular con 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora nos falta fittear nuestro algoritmo y calcular los resultados sobre el conjunto de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=pipeline, cv=kfold, param_grid=param_grid)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_n_components = best_params['reduce_dim__n_components']\n",
    "print(\"La mejor mejor configuración de parámetros es: \\n\" + \"Cantidad de componentes principales: \" + str(best_n_components))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez realizado esto, creamos los gráficos. Para esto creamos dos métodos, uno para la configuración del gráfico (es decir, necesitamos parsear los resultados obtenidos por PCA y KNN de manera que PlotLy los entienda y pueda graficar) y finalmente un método que nos permita graficar usando PlotLy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph_config(legends, results):\n",
    "    old_n_neighbors = legends[0]['knn__n_neighbors']\n",
    "    set_of_lists_with_results = []\n",
    "    list_with_results = []\n",
    "    graph_names = []\n",
    "    for index, legend in enumerate(legends):\n",
    "        new_n_neighbors = legend['knn__n_neighbors']\n",
    "\n",
    "        if new_n_neighbors == old_n_neighbors:\n",
    "            list_with_results.append(results[index])\n",
    "        else:\n",
    "            graph_names.append(old_n_neighbors)\n",
    "            set_of_lists_with_results.append(list_with_results)\n",
    "            list_with_results = [results[index]]\n",
    "            old_n_neighbors = new_n_neighbors\n",
    "    graph_names.append(new_n_neighbors)\n",
    "    set_of_lists_with_results.append(list_with_results)\n",
    "    return graph_names, set_of_lists_with_results\n",
    "\n",
    "def graph(N_COMPONENTS, graph_names, set_of_lists_with_results):\n",
    "    traces = []\n",
    "    for index, set in enumerate(set_of_lists_with_results):\n",
    "        x = N_COMPONENTS\n",
    "        y = set\n",
    "        name = \"cantidad de vecinos = \" + str(graph_names[index])\n",
    "        traces.append(Scatter(x=x, y=y, name=name))\n",
    "    layout = Layout(\n",
    "        xaxis=dict(\n",
    "            title='Cantidad de componentes principales',\n",
    "            type='log',\n",
    "            autorange=True\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='% Accuracy',\n",
    "            type='log',\n",
    "            autorange=True\n",
    "        ),\n",
    "        title=\"Medida de performance - Accuracy\"\n",
    "    )\n",
    "    figure = Figure(data=traces, layout=layout)\n",
    "    plotly.offline.iplot(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hecho esto, pasemos a graficar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#graph_names, set_of_lists_with_results = graph_config(legends, results)\n",
    "graph(N_COMPONENTS, [\"legend\"], [results])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-Gpu",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
